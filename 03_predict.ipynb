{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "service_account = 'renosterveld-ee@ee-vegetation-gee4geo.iam.gserviceaccount.com'\n",
    "credentials = ee.ServiceAccountCredentials(service_account, 'ee-vegetation-gee4geo-6309a79ef209.json')\n",
    "ee.Initialize(credentials)\n",
    "\n",
    "# EArth Engine docs:\n",
    "# https://developers.google.com/earth-engine/guides\n",
    "# https://gee-python-api.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from google.cloud import storage\n",
    "\n",
    "from utils.globals import *\n",
    "from utils.s2_ee import *\n",
    "from utils.misc_ee import *\n",
    "from utils.tf_data_utils import *\n",
    "from eoflow.models import TempCNNModel\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of satellite data and export to TFrecord\n",
    "These steps require no local compute. They just call the earth engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load area to predict\n",
    "predict = ee.FeatureCollection(PREDICT_MASK)\n",
    "\n",
    "#mask to our study area\n",
    "mask_predict = predict \\\n",
    "  .map(lambda feature: feature.set('flag', ee.Number(1))) \\\n",
    "  .reduceToImage(['flag'],ee.Reducer.first())\n",
    "\n",
    "#map to dates and create imagecol from results\n",
    "#produces stack of images from yesterday to 6 months ago\n",
    "ilist = pdates.map(predS2(predict,mask_predict))\n",
    "imageCol_gaps = ee.ImageCollection(ilist)\n",
    "imageCol_gaps = imageCol_gaps.select(allbands)\n",
    "names = imageCol_gaps.first().bandNames()\n",
    "\n",
    "#find first non-null value in timeseries\n",
    "first_im = imageCol_gaps\\\n",
    "  .reduce(ee.Reducer.firstNonNull())\\\n",
    "  .rename(names)\\\n",
    "  .set({'system:index': 'first'})\n",
    "first = ee.List([first_im])\n",
    "\n",
    "#fill nulls\n",
    "#then drop first image\n",
    "imageCol = ee.ImageCollection(ee.List(imageCol_gaps.iterate(gap_fill, first)))\\\n",
    "  .filter(ee.Filter.neq('system:index', 'first'))\n",
    "\n",
    "#timeseries length\n",
    "tsLengthPred = imageCol.size().getInfo()\n",
    "#18\n",
    "\n",
    "#flatten to array bands\n",
    "imageCol = imageCol.map(remclip(mask_predict))\n",
    "imageCol_all = imageCol.toArrayPerBand()\n",
    "\n",
    "\n",
    "#It may be necessary to save intermediate files to prevent exceeding ee capacity\n",
    "#Just ignore this if this entire code block succesfully exports\n",
    "\n",
    "#assetPath = 'users/glennwithtwons/s2_reno_predict'\n",
    "#str_name = 'task_phase3_'\n",
    "#task = ee.batch.Export.image.toAsset(image=imageCol_all,\n",
    "#                                 region=poi,\n",
    "#                                 description=str_name,\n",
    "#                                 assetId=assetPath,\n",
    "#                                 scale=10,\n",
    "#                                 maxPixels=2000000000000)\n",
    "#task.start()\n",
    "#task.status()\n",
    "#imageCol_all = ee.Image(assetPath)\n",
    "\n",
    "#define bands to export and dim\n",
    "patchdim = 64\n",
    "fmap = dict(zip(list(allbands), np.repeat(tsLengthPred,len(allbands)).tolist()))\n",
    "\n",
    "imageFilePrefix = PREDICT_IMG\n",
    "# Specify patch and file dimensions.\n",
    "imageExportFormatOptions = {\n",
    "  'patchDimensions': [patchdim, patchdim],\n",
    "  'maxFileSize': 104857600000,\n",
    "  'compressed': True,\n",
    "  'tensorDepths': fmap\n",
    "}\n",
    "\n",
    "# Export imagery in this region.\n",
    "exportRegion =  poi\n",
    "\n",
    "# Setup the task\n",
    "imageTask = ee.batch.Export.image.toCloudStorage(\n",
    "  image=imageCol_all,\n",
    "  description='Image Export',\n",
    "  fileNamePrefix=imageFilePrefix,\n",
    "  bucket=outputBucket,\n",
    "  maxPixels=200000000,\n",
    "  scale=10,\n",
    "  fileFormat='TFRecord',\n",
    "  region=exportRegion,\n",
    "  formatOptions=imageExportFormatOptions,\n",
    ")\n",
    "\n",
    "# Start the task\n",
    "imageTask.start()\n",
    "\n",
    "#Poll task status\n",
    "#imageTask.status()\n",
    "\n",
    "#this whole process can take 6-24 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "Here we read and preprocess the data exported by earth engine from a cloud storage bucket  \n",
    "We then predict using a saved model  \n",
    "  \n",
    "These steps require a GPU to accelerate predicition. It takes about 12-24hrs to finish predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precalculated scaling factors used to standardize data\n",
    "with open('data/max.json') as f:\n",
    "  Max = json.load(f)\n",
    "\n",
    "with open('data/min.json') as f:\n",
    "  Min = json.load(f)\n",
    "\n",
    "# Get a list of all the files in the output bucket.\n",
    "filesList = !gsutil ls 'gs://'{outputBucket}\n",
    "# Get only the files generated by the image export.\n",
    "exportFilesList = [s for s in filesList if imageFilePrefix in s]\n",
    "\n",
    "# Get the list of image files and the JSON mixer file.\n",
    "imageFilesList = []\n",
    "jsonFile = None\n",
    "for f in exportFilesList:\n",
    "  if f.endswith('.tfrecord.gz'):\n",
    "    imageFilesList.append(f)\n",
    "  elif f.endswith('.json'):\n",
    "    jsonFile = f\n",
    "\n",
    "#print(imageFilesList)\n",
    "#print(jsonFile)\n",
    "\n",
    "# Load the contents of the mixer file to a JSON object.\n",
    "jsonText = !gsutil cat {jsonFile}\n",
    "# Get a single string w/ newlines from the IPython.utils.text.SList\n",
    "mixer = json.loads(jsonText.nlstr)\n",
    "#print(mixer)\n",
    "\n",
    "# Get relevant info from the JSON mixer file.\n",
    "PATCH_WIDTH = mixer['patchDimensions'][0]\n",
    "PATCH_HEIGHT = mixer['patchDimensions'][1]\n",
    "PATCHES = mixer['totalPatches']\n",
    "PATCH_DIMENSIONS_FLAT = [tsLengthPred,PATCH_WIDTH * PATCH_HEIGHT]\n",
    "\n",
    "# Note that the tensors are in the shape of a patch, one patch for each band.\n",
    "imageColumns = [\n",
    "  tf.io.FixedLenFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32) \n",
    "    for k in allbands\n",
    "]\n",
    "\n",
    "# Parsing dictionary.\n",
    "imageFeaturesDict = dict(zip(allbands, imageColumns))\n",
    "\n",
    "# Note that you can make one dataset from many files by specifying a list.\n",
    "imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
    "\n",
    "# Parse the data into tensors, one long tensor per patch \n",
    "# then transpose\n",
    "# Break our long tensors into many little ones.\n",
    "# Turn the dictionary in each record into a tuple without a label.\n",
    "# if standaerdize needed it goes second last\n",
    "\n",
    "inDataset = imageDataset\\\n",
    ".map(lambda x: parse_image(x,imageFeaturesDict))\\\n",
    ".map(tp_image)\\\n",
    ".flat_map(\n",
    "  lambda features: tf.data.Dataset.from_tensor_slices(features)\n",
    ")\\\n",
    ".map(lambda x: standardize(x, Min,Max))\\\n",
    ".map(\n",
    "  lambda dataDict: (tf.transpose(list(dataDict.values())), )\n",
    ")\n",
    "\n",
    "# Turn each patch into a batch.\n",
    "inDataset = inDataset.batch(PATCH_WIDTH * PATCH_HEIGHT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction in batches, with as many steps as there are patches.\n",
    "model = tf.keras.models.load_model('save_test', compile=False)\n",
    "predictions = model.predict(inDataset, steps=PATCHES, verbose=1)\n",
    "\n",
    "# Note that the predictions come as a numpy array.  Check the first one.\n",
    "#print(predictions[0])\n",
    "pred_im = np.argmax(predictions,axis=1)\n",
    "#np.histogram(pred_im, bins=[0, 1, 2])\n",
    "outputImageFile = 'data/' + PREDICTIONS_IMG \n",
    "print('Writing to file ' + outputImageFile)\n",
    "\n",
    "# Instantiate the writer.\n",
    "writer = tf.io.TFRecordWriter(outputImageFile)\n",
    "\n",
    "# Every patch-worth of predictions we'll dump an example into the output\n",
    "# file with a single feature that holds our predictions. Since our predictions\n",
    "# are already in the order of the exported data, the patches we create here\n",
    "# will also be in the right order.\n",
    "patch = [[], [], []]\n",
    "curPatch = 1\n",
    "for prediction in predictions:\n",
    "  patch[0].append(tf.argmax(prediction))\n",
    "  patch[1].append(prediction[0])\n",
    "  patch[2].append(prediction[1])\n",
    "  # Once we've seen a patches-worth of class_ids...\n",
    "  if (len(patch[0]) == PATCH_WIDTH * PATCH_HEIGHT):\n",
    "    print('Done with patch ' + str(curPatch) + ' of ' + str(PATCHES) + '...')\n",
    "    # Create an example\n",
    "    example = tf.train.Example(\n",
    "      features=tf.train.Features(\n",
    "        feature={\n",
    "          'prediction': tf.train.Feature(\n",
    "              int64_list=tf.train.Int64List(\n",
    "                  value=patch[0])),\n",
    "          'RenoProb': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=patch[1])),\n",
    "          'TransProb': tf.train.Feature(\n",
    "              float_list=tf.train.FloatList(\n",
    "                  value=patch[2])),\n",
    "        }\n",
    "      )\n",
    "    )\n",
    "    # Write the example to the file and clear our patch array so it's ready for\n",
    "    # another batch of class ids\n",
    "    writer.write(example.SerializeToString())\n",
    "    patch = [[], [], []]\n",
    "    curPatch += 1\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload results to Earth Engine\n",
    "We need to send the results back to Earth Engine to turn them into a geospatial raster\n",
    "This does not require much compute - we are just uploading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload reasults from local storage to cloud storage\n",
    "#I am not sure this function works. I was getting permission errors\n",
    "#This should be v quick\n",
    "\n",
    "upload_blob(outputBucket_predict, outputImageFile, PREDICTIONS_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the files in the output bucket\n",
    "filesList = !gsutil ls 'gs://'{outputBucket_predict}\n",
    "# Get only the files generated by the image export.\n",
    "exportFilesList = [s for s in filesList if imageFilePrefix in s]\n",
    "\n",
    "jsonFile = None\n",
    "for f in exportFilesList:\n",
    "  if f.endswith('.json'):\n",
    "    jsonFile = f\n",
    "    \n",
    "print('Writing to ' + PREDICT_ASS)\n",
    "\n",
    "# Start the upload\n",
    "# calls earth engine from the command line\n",
    "# this takes a while - 6-12 hours\n",
    "# It is possible to poll the api to check the status of this task\n",
    "# ee.batch.Task.list()\n",
    "# ee.batch.Task(task_id).status\n",
    "\n",
    "# !earthengine upload image --asset_id={PREDICT_ASS} {'gs://' + outputBucket_predict+ '/' +PREDICTIONS_IMG} {jsonFile}"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
